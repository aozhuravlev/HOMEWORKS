### **Гребневая регрессия (Ridge Regression)**

**Ridge** — это метод линейной регрессии, который использует **L2 регуляризацию**. Ridge добавляет штраф за сумму квадратов коэффициентов модели. Основная цель гребневой регрессии — избежать слишком больших коэффициентов, которые могут привести к переобучению, особенно в ситуациях, когда признаки мультиколлинеарны (сильно коррелированы друг с другом).

### Формула:

Модель минимизирует следующую функцию:

$$
\text{Ridge} \text{ минимизирует } \left( \sum (y_i - X_i \beta)^2 + \lambda \sum \beta_j^2 \right)
$$

Где:
- $\lambda$ — коэффициент регуляризации. Чем больше $\lambda$, тем сильнее штраф, и тем меньше значения коэффициентов.

### Преимущества Ridge:

1. **Уменьшение переобучения:**
   Ridge помогает избежать переобучения, особенно когда в данных много признаков, и модель может подстраиваться под шум. Сжимая значения коэффициентов, модель становится более "гладкой" и устойчивой к шумам в данных.

2. **Стабилизация модели при мультиколлинеарности:**
   Когда признаки сильно коррелируют, обычная линейная регрессия может давать нестабильные и большие коэффициенты. Ridge помогает сгладить эти проблемы, уменьшив значения коэффициентов, не зануляя их.

3. **Использование всех признаков:**
   В отличие от LASSO, Ridge не зануляет коэффициенты полностью. Это означает, что все признаки будут использоваться в модели, но их влияние будет ограничено регуляризацией.

4. **Работа с данными, где число признаков больше числа наблюдений:**
   В задачах, где признаков больше, чем наблюдений (например, в задачах с большими данными), Ridge хорошо справляется, избегая переобучения за счет регуляризации.

### Пример с использованием Ridge

```python
import numpy as np
import pandas as pd
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Пример данных
data = pd.DataFrame({
    'GrLivArea': [1000, 1500, 1800, 2400, 3000, 3600],
    'GarageArea': [200, 300, 350, 450, 500, 600],
    'SalePrice': [150000, 200000, 220000, 300000, 360000, 400000]
})

X = data[['GrLivArea', 'GarageArea']]
y = data['SalePrice']

# Разделение данных на тренировочные и тестовые
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Обучение модели Ridge
ridge = Ridge(alpha=0.1)  # alpha — это коэффициент регуляризации
ridge.fit(X_train, y_train)

# Предсказания
y_pred = ridge.predict(X_test)

# Оценка точности (RMSE)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"RMSE Ridge: {rmse}")

# Коэффициенты модели
print(f"Коэффициенты Ridge: {ridge.coef_}")
print(f"Свободный член (intercept): {ridge.intercept_}")

# Построение графика предсказанных vs фактических значений
plt.scatter(y_test, y_pred)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red')
plt.xlabel('Фактические значения')
plt.ylabel('Предсказанные значения')
plt.title('Ridge: Предсказанные vs Фактические значения')
plt.show()
```

### Подбор параметра регуляризации (λ):

Для Ridge важно правильно выбрать коэффициент регуляризации **$\lambda$**. Чем больше значение **$\lambda$**, тем сильнее штраф и меньше коэффициенты. Однако слишком большое значение может ухудшить точность предсказаний.

Мы можем использовать кросс-валидацию для подбора оптимального значения **\(\lambda\)**.

```python
from sklearn.model_selection import GridSearchCV

# Подбор параметра alpha для Ridge
param_grid = {'alpha': np.linspace(0.01, 1, 10)}
ridge_cv = GridSearchCV(Ridge(), param_grid, cv=5)
ridge_cv.fit(X_train, y_train)

# Оптимальный параметр alpha
print(f"Оптимальный alpha для Ridge: {ridge_cv.best_params_['alpha']}")
```

### Преимущества Ridge в сравнении с LASSO и Elastic Net:
- **Использование всех признаков:** В отличие от LASSO, Ridge не зануляет коэффициенты признаков. Это особенно полезно, если все признаки потенциально важны для задачи.
- **Стабилизация при мультиколлинеарности:** Ridge снижает влияние мультиколлинеарности, сжимая значения коэффициентов, что делает модель более стабильной.
- **Подходит для высокоразмерных данных:** Ridge успешно работает, когда количество признаков превышает количество наблюдений, предотвращая переобучение.

### Когда использовать Ridge?
Ridge стоит использовать, если тебе нужно удержать все признаки в модели, но при этом снизить влияние некоторых из них (особенно при наличии мультиколлинеарности). Если все признаки потенциально важны для задачи, и их нельзя полностью исключить, Ridge — отличный выбор для стабилизации модели и предотвращения переобучения.