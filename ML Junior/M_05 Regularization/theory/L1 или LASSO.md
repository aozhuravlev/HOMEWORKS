### **LASSO (Least Absolute Shrinkage and Selection Operator)**

**LASSO** — это метод линейной регрессии, который использует **L1 регуляризацию**. Он добавляет штраф за сумму абсолютных значений коэффициентов модели. Основная особенность LASSO заключается в том, что он может занулять некоторые коэффициенты, фактически исключая незначимые признаки из модели, что помогает в отборе признаков.

### Формула:

Модель минимизирует следующую функцию:

$$
\text{LASSO} \text{ минимизирует } \left( \sum (y_i - X_i \beta)^2 + \lambda \sum |\beta_j| \right)
$$

Где:
- $\lambda$ — коэффициент регуляризации. Чем больше $\lambda$, тем сильнее штраф, и тем больше коэффициентов будут стремиться к нулю.

### Преимущества LASSO:

1. **Автоматический отбор признаков:**
   LASSO может занулять некоторые коэффициенты признаков, что позволяет исключить из модели неинформативные или менее значимые признаки. Это особенно полезно при работе с высокоразмерными данными (когда признаков много).

2. **Снижение переобучения:**
   Добавление L1 штрафа помогает предотвратить переобучение, уменьшая величину коэффициентов и делая модель менее сложной.

3. **Простота интерпретации:**
   За счет зануления некоторых коэффициентов модель становится проще для интерпретации, так как использует меньшее количество признаков.

4. **Хорошо работает при высокой мультиколлинеарности:**
   В случае, когда признаки сильно коррелируют друг с другом, LASSO помогает выбрать один из них, занулив остальные. Это делает модель более стабильной и легко интерпретируемой.

### Пример с использованием LASSO

```python
import numpy as np
import pandas as pd
from sklearn.linear_model import Lasso
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Пример данных
data = pd.DataFrame({
    'GrLivArea': [1000, 1500, 1800, 2400, 3000, 3600],
    'GarageArea': [200, 300, 350, 450, 500, 600],
    'SalePrice': [150000, 200000, 220000, 300000, 360000, 400000]
})

X = data[['GrLivArea', 'GarageArea']]
y = data['SalePrice']

# Разделение данных на тренировочные и тестовые
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Обучение модели LASSO
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)

# Предсказания
y_pred = lasso.predict(X_test)

# Оценка точности (RMSE)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"RMSE LASSO: {rmse}")

# Коэффициенты модели
print(f"Коэффициенты LASSO: {lasso.coef_}")
print(f"Свободный член (intercept): {lasso.intercept_}")

# Построение графика предсказанных vs фактических значений
plt.scatter(y_test, y_pred)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red')
plt.xlabel('Фактические значения')
plt.ylabel('Предсказанные значения')
plt.title('LASSO: Предсказанные vs Фактические значения')
plt.show()
```

### Подбор параметра регуляризации (λ):

Для LASSO важно правильно выбрать коэффициент регуляризации **$\lambda$**. Большое значение **$\lambda$** может привести к тому, что модель станет слишком простой и занулит многие коэффициенты, а слишком маленькое значение не поможет предотвратить переобучение.

Мы можем использовать кросс-валидацию для подбора оптимального значения **$\lambda$** (или параметра **alpha** в реализации sklearn).

```python
from sklearn.model_selection import GridSearchCV

# Подбор параметра alpha для LASSO
param_grid = {'alpha': np.linspace(0.01, 1, 10)}
lasso_cv = GridSearchCV(Lasso(), param_grid, cv=5)
lasso_cv.fit(X_train, y_train)

# Оптимальный параметр alpha
print(f"Оптимальный alpha для LASSO: {lasso_cv.best_params_['alpha']}")
```

### Преимущества LASSO в сравнении с Ridge и Elastic Net:
- **Зануление коэффициентов:** В отличие от Ridge (гребневой регрессии), где коэффициенты просто уменьшаются, LASSO может полностью занулить незначимые признаки. Это особенно полезно для отбора признаков.
- **Меньшая сложность:** За счет зануления коэффициентов модель становится проще и легче для интерпретации.
- **Удобен для данных с множеством признаков:** LASSO может исключить менее значимые признаки, что особенно полезно при работе с высокоразмерными данными.

### Когда использовать LASSO?
LASSO особенно полезен, если у тебя много признаков, и ты подозреваешь, что некоторые из них не оказывают значимого влияния на целевую переменную. Если нужно одновременно уменьшить сложность модели и выбрать наиболее важные признаки, LASSO — отличный выбор.