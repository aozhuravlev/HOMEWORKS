{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Инфраструктура для моделей машинного обучения. Практическая работа\n","\n","# Цель практической работы\n","\n","Потренироваться в использовании библиотек PySpark SQL и PySpark ML для предобработки данных и обучения моделей.\n","\n","# Что входит в практическую работу\n","\n","1. Инициализация спарк-сессии.\n","2. Загрузка данных.\n","3. Ознакомление с данными.\n","4. Преобразование типов столбцов.\n","5. Очистка данных.\n","6. Feature-инжиниринг.\n","7. Векторизация фичей.\n","8. Создание и обучение модели.\n","9. Выбор лучшей модели.\n","10. Обратная связь.\n","\n","\n","# Что оценивается\n","\n","- Пройдены все этапы работы.\n","- Спарк-сессия успешно запущена.\n","- Данные прочитаны.\n","- Все колонки с числовыми значениями преобразованы в числовые типы данных (Int или Double).\n","- Отфильтрованы все строки с Null-значениями.\n","- Созданы новые фичи.\n","- Все категориальные колонки преобразованы в числовой вид, выполнены все этапы векторизации признаков.\n","- Выборка разделена на обучающую и тестовую.\n","- Создано три объекта: модель, сетка гиперпараметров и evaluator.\n","- Создан объект класса CrossValidator и обучен на обучающей выборке.\n","- Выбрана лучшая модель, посчитана метрика качества лучшей модели.\n","\n","\n","# Задача\n","\n","Используя данные о клиентах телекоммуникационной компании, обучите модель, предсказывающую их отток.\n","\n","Описание данных, с которыми вы будете работать:\n","\n","* **CustomerID**: ID клиента.\n","* **Gender**: пол клиента.\n","* **SeniorCitizen**: пенсионер ли клиент (1 — да, 0 — нет).\n","* **Partner**: есть у клиента партнёр (жена, муж) или нет (Yes/No).\n","* **Dependents**: есть ли у клиента инждивенцы, например дети (Yes/No).\n","* **Tenure**: как много месяцев клиент оставался в компании.\n","* **PhoneService**: подключена ли у клиента телефонная служба (Yes/No).\n","* **MultipleLines**: подключено ли несколько телефонных линий (Yes, No, No phone service).\n","* **InternetService**: интернет-провайдер клиента (DSL, Fiber optic, No).\n","* **OnlineSecurity**: подключена ли у клиента услуга онлайн-безопасности (Yes, No, No internet service)\n","* **OnlineBackup**: подключена ли услуга резервного копирования онлайн (Yes, No, No internet service).\n","* **DeviceProtection**: подключена ли услуга защиты устройства (Yes, No, No internet service)\n","* **TechSupport**: есть ли у клиента техническая поддержка (Yes, No, No internet service).\n","* **StreamingTV**: подключена ли услуга потокового телевидения (Yes, No, No internet service).\n","* **StreamingMovies**: подключена ли услуга стримингового воспроизведения фильмов (Yes, No, No internet service).\n","* **Contract**: тип контракта клиента (Month-to-month, One year, Two year).\n","* **PaperlessBilling**: есть ли безбумажный счёт.\n","* **PaymentMethod**: способ оплаты услуг (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic)).\n","* **MonthlyCharges**: сумма, которая списывается ежемесячно.\n","* **TotalCharges**: сумма, списанная за всё время.\n","* **Churn**: ушёл ли клиент (Yes/No). Это целевая переменная, которую нужно предсказать.\n"],"metadata":{"id":"p92rzC-2D71g"}},{"cell_type":"markdown","source":["# 1. Инициализация спарк-сессии"],"metadata":{"id":"CZmr6VeYvLKB"}},{"cell_type":"markdown","source":["Инициализируйте спарк-сессию.\n","\n","Эта ячейка нужна для того, чтобы заргузить необходимые библиотеки и настроить окружение Google Colab для работы со Spark.\n","\n","Просто запустите её перед выполением задания :)"],"metadata":{"id":"EO2v2cOjyZBN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UuUc1cxHuymb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682669062099,"user_tz":-180,"elapsed":95712,"user":{"displayName":"Kirill Bogdanov","userId":"01175823895624937540"}},"outputId":"b14a1c63-36ca-4413-dc2d-59ccbe9799ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install pyspark --quiet\n","!pip install -U -q PyDrive --quiet\n","!apt install openjdk-8-jdk-headless &> /dev/null\n","\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","\n","!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip &> /dev/null\n","!unzip ngrok-stable-linux-amd64.zip &> /dev/null\n","get_ipython().system_raw('./ngrok http 4050 &')\n"]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","\n","### Ваш код здесь ###"],"metadata":{"id":"8Q743I78vA5_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. Загрузка данных\n","Загрузите данные, сохраните их в переменную типа sparkDataframe, используя метод read.csv (не забывайте про header и delimiter)."],"metadata":{"id":"e55O3aB5vo3k"}},{"cell_type":"code","source":["### Ваш код здесь ###"],"metadata":{"id":"1GVOybUCvlXy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Ознакомление с данными\n","1. Выведите на экран первые несколько строк датафрейма.\n"],"metadata":{"id":"k8amaWNGzDsf"}},{"cell_type":"code","source":["### Ваш код здесь ###"],"metadata":{"id":"bPfkFFUoz19Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682661059903,"user_tz":-180,"elapsed":1244,"user":{"displayName":"Kirill Bogdanov","userId":"01175823895624937540"}},"outputId":"38a7e925-7241-42c1-b490-018f6a6f2eb6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+--------------------+--------------+------------+-----+\n","|customerID|gender|SeniorCitizen|Partner|Dependents|tenure|PhoneService|   MultipleLines|InternetService|OnlineSecurity|OnlineBackup|DeviceProtection|TechSupport|StreamingTV|StreamingMovies|      Contract|PaperlessBilling|       PaymentMethod|MonthlyCharges|TotalCharges|Churn|\n","+----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+--------------------+--------------+------------+-----+\n","|7590-VHVEG|Female|            0|    Yes|        No|     1|          No|No phone service|            DSL|            No|         Yes|              No|         No|         No|             No|Month-to-month|             Yes|    Electronic check|         29.85|       29.85|   No|\n","|5575-GNVDE|  Male|            0|     No|        No|    34|         Yes|              No|            DSL|           Yes|          No|             Yes|         No|         No|             No|      One year|              No|        Mailed check|         56.95|      1889.5|   No|\n","|3668-QPYBK|  Male|            0|     No|        No|     2|         Yes|              No|            DSL|           Yes|         Yes|              No|         No|         No|             No|Month-to-month|             Yes|        Mailed check|         53.85|      108.15|  Yes|\n","|7795-CFOCW|  Male|            0|     No|        No|    45|          No|No phone service|            DSL|           Yes|          No|             Yes|        Yes|         No|             No|      One year|              No|Bank transfer (au...|          42.3|     1840.75|   No|\n","|9237-HQITU|Female|            0|     No|        No|     2|         Yes|              No|    Fiber optic|            No|          No|              No|         No|         No|             No|Month-to-month|             Yes|    Electronic check|          70.7|      151.65|  Yes|\n","+----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+--------------------+--------------+------------+-----+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"markdown","source":["\n","2. Выведите общее количество строк датафрейма.\n","\n"],"metadata":{"id":"vMgX25wr0JfV"}},{"cell_type":"code","source":["### Ваш код здесь ###"],"metadata":{"id":"ivgw172qz9o_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Выведите структуру (схему) датафрейма."],"metadata":{"id":"xCAeIZFe0KyY"}},{"cell_type":"code","source":["### Ваш код здесь ###"],"metadata":{"id":"KmZY25zX0Fd5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. Преобразование типов столбцов\n","Преобразуйте тип столбцов у числовых признаков (Int — если признак целочисленный, Double — если признак не целочисленный). Сохраните преобразованный датафрейм в новую переменную.\n","\n","## Совет\n","\n","Если вам сложно выполнить это задание, изучите дополнительные материалы: [об операторе Cast](https://sparkbyexamples.com/pyspark/pyspark-cast-column-type/), [об операторе Select](https://sparkbyexamples.com/pyspark/select-columns-from-pyspark-dataframe/).\n","\n"],"metadata":{"id":"DFcKPAI_0cF7"}},{"cell_type":"code","source":["from pyspark.sql.functions import expr, col\n","\n","### Ваш код здесь ###"],"metadata":{"id":"Kihtrqni0-Js"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. Очистка данных\n","Проверьте, есть ли в какой-либо колонке Null-значения. Для этого можно использовать your_dataframe.filter(col(\"colname\")).isNull()).\n","\n","Выведите на экран несколько строк с Null-значениями в одной из колонок.\n","\n","Сохраните очищенный от строк с Null-значениями датафрейм в новую переменную. Для фильтрации этих значений можно использовать метод .isNotNull().\n","\n","Колонок в датафрейме много, проверять каждую неудобно и долго. Подумайте, как упроситить эту работу, если использовать, например, перебор с циклом for.\n","\n","[Примеры использования операторов isNull() и isNotNull()](https://sparkbyexamples.com/pyspark/pyspark-isnull/).\n"],"metadata":{"id":"1hBBiIm350BD"}},{"cell_type":"code","source":["### Ваш код здесь ###"],"metadata":{"id":"A0FTVvpg6_iy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6. Feature-инжиниринг\n","Добавьте в датафрейм одну или несколько новых фичей. Удалите колонки, которые, как вам кажется, нужно убрать из фичей. Обоснуйте свои решения."],"metadata":{"id":"fMRcAbphNBEP"}},{"cell_type":"code","source":["from pyspark.sql.functions import when\n","\n","### Ваш код здесь ###"],"metadata":{"id":"6SZ_rocx79oY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#7. Векторизация фичей\n","Подготовьте данные к обучению:\n","\n","\n","\n"],"metadata":{"id":"mX-4GEdHTZWd"}},{"cell_type":"markdown","source":["1. Преобразуйте текстовые колонки в числа, используя StringIndexer.\n","Удалите столбцы со старыми (непреобразованными) признаками. Выведите на экран структуру получившегося датафрейма. Не забывайте о столбце Churn. Хоть он и выступает в задаче как таргет, он имеет текстовый тип, поэтому тоже должен быть закодирован числовыми значениями.\n","\n","Чтобы использовать StringIndexer для всех категориальных признаков сразу, а не для каждого отдельно, можно применить сущность pipeline.\n","\n","**Пример кода:**\n","\n","##### #Задаём список текстовых колонок:\n","text_columns = [\"text_col_1\", \"text_col_2\", \"text_col_3\"]\n","\n","##### #Задаём список StringIndexer'ов — сущностей, каждая из которых будет кодировать одну текстовую колонку числами. Имена преобразованных колонок будут заканчиваться на _index:\n","indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\",).fit(<ваш датасет>) for column in text_columns]\n","\n","##### #Создаём Pipeline из StringIndexer'ов:\n","pipeline = Pipeline(stages=indexers)\n","\n","##### #Скармливаем нашему pipeline датафрейм, удаляя старые колонки:\n","new_dataframe = pipeline.fit(<ваш датасет>).transform(<ваш датасет>).drop(*text_columns)\n"],"metadata":{"id":"x5cNxN2qq3SL"}},{"cell_type":"code","source":["from pyspark import mllib\n","from pyspark.ml.feature import StringIndexer\n","from pyspark.ml import Pipeline\n","\n","#список колонок с текстовым типом\n","text_cols = [\"gender\", \"Partner\", \"Dependents\", \"PhoneService\", \"MultipleLines\", \"InternetService\", \\\n","                \"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \\\n","                \"StreamingMovies\", \"Contract\", \"PaperlessBilling\", \"PaymentMethod\", \"Churn\"]\n","\n","\n","### Ваш код здесь ###"],"metadata":{"id":"0XQ3pjdAcBIL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Векторизуйте категориальные признаки, используя OneHotEncoder.\n","Удалите столбцы со старыми (непреобразованными) признаками.\n","Выведите на экран структуру получившегося после преобразований датафрейма.\n"],"metadata":{"id":"EEqoj7IcmB5H"}},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoder\n","\n","#список категориальных колонок\n","features_inp  = [\"gender\", \"Partner\", \"Dependents\", \"PhoneService\", \"MultipleLines\", \"InternetService\", \\\n","                \"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \\\n","                \"StreamingMovies\", \"Contract\", \"PaperlessBilling\", \"PaymentMethod\"]\n","\n","\n","### Ваш код здесь ###"],"metadata":{"id":"Gihv_Q00TkOD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Объедините колонки фичей в один вектор, используя VectorAssembler.\n","Удалите столбцы со старыми (непреобразованными) признаками.\n","Выведите на экран первые несколько строк и структуру получившегося датафрейма."],"metadata":{"id":"WscurK1IoJ8w"}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n","\n","\n","### Ваш код здесь ###"],"metadata":{"id":"wOm-Te9bYMia"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#8. Создание и обучение модели"],"metadata":{"id":"uk_1ZJjDqm02"}},{"cell_type":"markdown","source":["1. Создайте модель — логистическую регрессию (используя LogisticRegression). В качестве параметров класса LogisticRegression укажите колонку фичей (параметр featuresCol), колонку-таргет (параметр labelCol) из датафрейма и имя колонки, в которую будут записываться предсказания (параметр predictionCol)."],"metadata":{"id":"fMsi-JYVqvkX"}},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n","\n","### Ваш код здесь ###"],"metadata":{"id":"siYFALNhq_9F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Разделите датафрейм на обучающую и тестовую выборку."],"metadata":{"id":"OCG0euR8v33u"}},{"cell_type":"code","source":["\n","### Ваш код здесь ###"],"metadata":{"id":"g9PoHvGqvvET"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Создайте объекты — сетки гиперпараметров для каждой модели, используя ParamGridBuilder. Так же, как и в ноутбуке из последнего видео, в сетку гиперпараметров можно добавить значения параметров regParam и elasticNetParam.\n","\n","Вы можете ознакомиться [с документацией объекта LogisticRegression в PySpark](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.LogisticRegression.html) и добавить в сетку больше параметров.\n"],"metadata":{"id":"MyGEK9T_wGWd"}},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder\n","\n","### Ваш код здесь ###"],"metadata":{"id":"LsRwAy1TwDzF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4. Создайте объект evaluator, который будет отвечать за метрику качества при обучении. Для этого используйте класс BinaryClassificationEvaluator со следующими параметрами: rawPredictionCol — колонка с предсказаниями, labelCol — колонка с таргетом.\n","\n","У вас, возможно, возник вопрос, какую метрику качества берёт по умолчанию BinaryClassificationEvaluator. По умолчанию BinaryClassificationEvaluator будет рассчитывать areaUnderROC. Это метрика оценки площади под кривой ROC (Receiver Operating Characteristic), которая служит графической интерпретацией производительности модели. Эта метрика качества находится в пределах от 0 до 1. Чем выше метрика, тем более качественные предсказания делает модель."],"metadata":{"id":"zg0lwkdoJjaQ"}},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","\n","### Ваш код здесь ###"],"metadata":{"id":"RrGPelUqMeoz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5. Создайте объект CrossValidator, в качестве параметров укажите уже созданные вами модель, сетку гиперпараметров и evaluator."],"metadata":{"id":"7urf-OeRMTdC"}},{"cell_type":"code","source":["from pyspark.ml.tuning import CrossValidator\n","\n","### Ваш код здесь ###"],"metadata":{"id":"CtmsG_rxSOOr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6. Запустите обучение модели на тренировочной выборке. Сохраните обученную модель в новую переменную."],"metadata":{"id":"Jmpv70TjZByl"}},{"cell_type":"code","source":["\n","### Ваш код здесь ###"],"metadata":{"id":"uUxWevjEYOvX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#9. Выбор лучшей модели"],"metadata":{"id":"WaIi8In5Zykn"}},{"cell_type":"markdown","source":["1. Выберите лучшую модель, сохраните её в отдельную переменную, отобразите её параметры.\n","\n","Вывод параметров модели в PySpark можно сделать, используя метод extractParamMap()."],"metadata":{"id":"29eo5S_KZL4k"}},{"cell_type":"code","source":["\n","### Ваш код здесь ###"],"metadata":{"id":"kJK-IrWLZbL-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Запустите лучшую модель в режиме предсказания на тестовой выборке. Сохраните предсказания в отдельную переменную. Выведите первые несколько строк датафрейма с предсказаниями на экран.\n","\n","Запуск модели в режиме предсказания выполняется при помощи метода .transform(<тестовая выборка>)."],"metadata":{"id":"1BIe8gqugBds"}},{"cell_type":"code","source":["\n","### Ваш код здесь ###"],"metadata":{"id":"_K1IrbNzd1FX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Получите метрику качества модели. Для этого примените к объекту evaluator метод .evaluate(<ваш датафрейм с предсказаниями>).\n","\n"],"metadata":{"id":"gC41fZbahGCB"}},{"cell_type":"code","source":["\n","### Ваш код здесь ###"],"metadata":{"id":"CBzDaleQhejy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#10. Обратная связь\n","Вы ознакомились с возможностями двух мощных библиотек: PySpark SQL для предобработки данных и PySpark ML для машинного обучения."],"metadata":{"id":"nieC-XnAl-4m"}},{"cell_type":"markdown","source":["Поделитесь впечатлениями от работы с новыми библиотеками. В чём они более удобны, чем уже знакомые вам Pandas и Sklearn, а в чём нет."],"metadata":{"id":"axLO-xV3pJx2"}},{"cell_type":"markdown","source":["# Как отправить работу на проверку\n","\n","Загрузите файл с заданиями, откройте его через Jupyter Notebook в Google Colab. Скачайте файл с датасетом и загрузите его в Colab. Выполните задачи, сохраните изменения: воспользуйтесь опцией Save and Checkpoint из вкладки меню File или кнопкой Save and Checkpoint на панели инструментов. Отправьте через форму ниже итоговый файл Jupyter Notebook (.ipynb) или ссылку на него."],"metadata":{"id":"V9TYdK5gl9P4"}}]}