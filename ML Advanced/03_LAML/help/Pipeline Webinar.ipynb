{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ao0HuUaYgeR",
        "outputId": "62a4b651-9b77-43d6-dbc7-8ca064b852b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightautoml\n",
            "  Downloading lightautoml-0.3.8.1-py3-none-any.whl (416 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.4/416.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autowoe>=1.2 (from lightautoml)\n",
            "  Downloading AutoWoE-1.3.2-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.7/215.7 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting catboost>=0.26.1 (from lightautoml)\n",
            "  Downloading catboost-1.2.3-cp310-cp310-manylinux2014_x86_64.whl (98.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmaes (from lightautoml)\n",
            "  Downloading cmaes-0.10.0-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: holidays in /usr/local/lib/python3.10/dist-packages (from lightautoml) (0.44)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from lightautoml) (3.1.3)\n",
            "Collecting joblib<1.3.0 (from lightautoml)\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting json2html (from lightautoml)\n",
            "  Downloading json2html-1.3.0.tar.gz (7.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lightgbm<=3.2.1,>=2.3 (from lightautoml)\n",
            "  Downloading lightgbm-3.2.1-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from lightautoml) (3.2.1)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from lightautoml) (1.25.2)\n",
            "Collecting optuna (from lightautoml)\n",
            "  Downloading optuna-3.6.0-py3-none-any.whl (379 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m379.9/379.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas<2.0.0 in /usr/local/lib/python3.10/dist-packages (from lightautoml) (1.5.3)\n",
            "Collecting poetry-core<2.0.0,>=1.0.0 (from lightautoml)\n",
            "  Downloading poetry_core-1.9.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.5/309.5 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from lightautoml) (6.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from lightautoml) (1.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from lightautoml) (0.13.1)\n",
            "Collecting statsmodels<=0.14.0 (from lightautoml)\n",
            "  Downloading statsmodels-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch<=2.0.0,>=1.9.0 (from lightautoml)\n",
            "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lightautoml) (4.66.2)\n",
            "Collecting StrEnum<0.5.0,>=0.4.7 (from autowoe>=1.2->lightautoml)\n",
            "  Downloading StrEnum-0.4.15-py3-none-any.whl (8.9 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from autowoe>=1.2->lightautoml) (3.7.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from autowoe>=1.2->lightautoml) (7.4.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from autowoe>=1.2->lightautoml) (2023.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from autowoe>=1.2->lightautoml) (1.11.4)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.10/dist-packages (from autowoe>=1.2->lightautoml) (5.0.2)\n",
            "Collecting sphinx-rtd-theme (from autowoe>=1.2->lightautoml)\n",
            "  Downloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost>=0.26.1->lightautoml) (0.20.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost>=0.26.1->lightautoml) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost>=0.26.1->lightautoml) (1.16.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from lightgbm<=3.2.1,>=2.3->lightautoml) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0.0->lightautoml) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->lightautoml) (3.3.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels<=0.14.0->lightautoml) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels<=0.14.0->lightautoml) (24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<=2.0.0,>=1.9.0->lightautoml) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<=2.0.0,>=1.9.0->lightautoml) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<=2.0.0,>=1.9.0->lightautoml) (1.12)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch<=2.0.0,>=1.9.0->lightautoml)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch<=2.0.0,>=1.9.0->lightautoml)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch<=2.0.0,>=1.9.0->lightautoml)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch<=2.0.0,>=1.9.0->lightautoml)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch<=2.0.0,>=1.9.0->lightautoml)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch<=2.0.0,>=1.9.0->lightautoml)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch<=2.0.0,>=1.9.0->lightautoml)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch<=2.0.0,>=1.9.0->lightautoml)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch<=2.0.0,>=1.9.0->lightautoml)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch<=2.0.0,>=1.9.0->lightautoml)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch<=2.0.0,>=1.9.0->lightautoml)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0 (from torch<=2.0.0,>=1.9.0->lightautoml)\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<=2.0.0,>=1.9.0->lightautoml) (67.7.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<=2.0.0,>=1.9.0->lightautoml) (3.27.9)\n",
            "Collecting lit (from triton==2.0.0->torch<=2.0.0,>=1.9.0->lightautoml)\n",
            "  Downloading lit-18.1.1.tar.gz (161 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.1/161.1 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->lightautoml) (2.1.5)\n",
            "Collecting alembic>=1.5.0 (from optuna->lightautoml)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna->lightautoml)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna->lightautoml) (2.0.28)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna->lightautoml)\n",
            "  Downloading Mako-1.3.2-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autowoe>=1.2->lightautoml) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autowoe>=1.2->lightautoml) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autowoe>=1.2->lightautoml) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autowoe>=1.2->lightautoml) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autowoe>=1.2->lightautoml) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autowoe>=1.2->lightautoml) (3.1.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna->lightautoml) (3.0.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost>=0.26.1->lightautoml) (8.2.3)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->autowoe>=1.2->lightautoml) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->autowoe>=1.2->lightautoml) (1.4.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->autowoe>=1.2->lightautoml) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->autowoe>=1.2->lightautoml) (2.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (1.0.8)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (1.0.6)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (2.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (1.1.10)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (1.0.7)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (2.16.1)\n",
            "Requirement already satisfied: docutils<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (0.18.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (2.14.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (2.31.0)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->autowoe>=1.2->lightautoml)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<=2.0.0,>=1.9.0->lightautoml) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx->autowoe>=1.2->lightautoml) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx->autowoe>=1.2->lightautoml) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx->autowoe>=1.2->lightautoml) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx->autowoe>=1.2->lightautoml) (2024.2.2)\n",
            "Building wheels for collected packages: json2html, lit\n",
            "  Building wheel for json2html (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for json2html: filename=json2html-1.3.0-py3-none-any.whl size=7593 sha256=27fee6427d80805e6c981e6014c296d1a6ac7bfb01016dcb383168f494ca99cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/d8/b3/6f83a04ab0ec00e691de794d108286bb0f8bcdf4ade19afb57\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-18.1.1-py3-none-any.whl size=96363 sha256=e0c44ded20d50971fab9ee4afdb7847834718f7ec7742d57824c3b981efc485a\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/74/6b/88e95944e9f9078f1dc1c0f634a542efb4d26ecae6000ca8cf\n",
            "Successfully built json2html lit\n",
            "Installing collected packages: StrEnum, lit, json2html, poetry-core, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, Mako, joblib, colorlog, cmaes, nvidia-cusolver-cu11, nvidia-cudnn-cu11, alembic, statsmodels, sphinxcontrib-jquery, optuna, lightgbm, catboost, sphinx-rtd-theme, autowoe, triton, torch, lightautoml\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.3.2\n",
            "    Uninstalling joblib-1.3.2:\n",
            "      Successfully uninstalled joblib-1.3.2\n",
            "  Attempting uninstall: statsmodels\n",
            "    Found existing installation: statsmodels 0.14.1\n",
            "    Uninstalling statsmodels-0.14.1:\n",
            "      Successfully uninstalled statsmodels-0.14.1\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 4.1.0\n",
            "    Uninstalling lightgbm-4.1.0:\n",
            "      Successfully uninstalled lightgbm-4.1.0\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.2.0\n",
            "    Uninstalling triton-2.2.0:\n",
            "      Successfully uninstalled triton-2.2.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.1+cu121\n",
            "    Uninstalling torch-2.2.1+cu121:\n",
            "      Successfully uninstalled torch-2.2.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.3.2 StrEnum-0.4.15 alembic-1.13.1 autowoe-1.3.2 catboost-1.2.3 cmaes-0.10.0 colorlog-6.8.2 joblib-1.2.0 json2html-1.3.0 lightautoml-0.3.8.1 lightgbm-3.2.1 lit-18.1.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 optuna-3.6.0 poetry-core-1.9.0 sphinx-rtd-theme-2.0.0 sphinxcontrib-jquery-4.1 statsmodels-0.14.0 torch-2.0.0 triton-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U lightautoml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard python libraries\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "\n",
        "\n",
        "# Installed libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "# import torch\n",
        "\n",
        "# Imports from our package\n",
        "from lightautoml.automl.base import AutoML\n",
        "from lightautoml.ml_algo.boost_lgbm import BoostLGBM\n",
        "from lightautoml.ml_algo.boost_cb import BoostCB\n",
        "from lightautoml.ml_algo.tuning.optuna import OptunaTuner\n",
        "from lightautoml.pipelines.features.lgb_pipeline import LGBSimpleFeatures\n",
        "from lightautoml.pipelines.ml.base import MLPipeline\n",
        "from lightautoml.pipelines.selection.importance_based import ImportanceCutoffSelector, ModelBasedImportanceEstimator\n",
        "from lightautoml.reader.base import PandasToPandasReader\n",
        "from lightautoml.tasks import Task\n",
        "from lightautoml.automl.blend import WeightedBlender"
      ],
      "metadata": {
        "id": "ijsfmKizYjYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_THREADS = 8 # threads cnt for lgbm and linear models\n",
        "N_FOLDS = 5 # folds cnt for AutoML\n",
        "RANDOM_STATE = 42 # fixed random state for various reasons\n",
        "TEST_SIZE = 0.2 # Test size for metric check\n",
        "TARGET_NAME = 'TARGET' # Target column name\n",
        "\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "\n",
        "DATASET_DIR = '../data/'\n",
        "DATASET_NAME = 'sampled_app_train.csv'\n",
        "DATASET_FULLNAME = os.path.join(DATASET_DIR, DATASET_NAME)\n",
        "DATASET_URL = 'https://raw.githubusercontent.com/sberbank-ai-lab/LightAutoML/master/examples/data/sampled_app_train.csv'"
      ],
      "metadata": {
        "id": "fEeG8T8aYmHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "if not os.path.exists(DATASET_FULLNAME):\n",
        "    os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "\n",
        "    dataset = requests.get(DATASET_URL).text\n",
        "    with open(DATASET_FULLNAME, 'w') as output:\n",
        "        output.write(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPe5TOMuYmmo",
        "outputId": "d2cbbda0-29af-4a4b-b510-b59cfa3bd2ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 42 µs, sys: 0 ns, total: 42 µs\n",
            "Wall time: 80.6 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(DATASET_FULLNAME)\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "Ssp60PhFZumr",
        "outputId": "bbcf75d7-0d36-436f-f320-1834c70b613e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   SK_ID_CURR  TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR  \\\n",
              "0      313802       0         Cash loans           M            N   \n",
              "1      319656       0         Cash loans           F            N   \n",
              "2      207678       0    Revolving loans           F            Y   \n",
              "3      381593       0         Cash loans           F            N   \n",
              "4      258153       0         Cash loans           F            Y   \n",
              "\n",
              "  FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
              "0               Y             0          270000.0    327024.0      15372.0   \n",
              "1               N             0          108000.0    675000.0      19737.0   \n",
              "2               Y             2          112500.0    270000.0      13500.0   \n",
              "3               N             1           67500.0    142200.0       9630.0   \n",
              "4               Y             0          337500.0   1483231.5      46570.5   \n",
              "\n",
              "   ...  FLAG_DOCUMENT_18 FLAG_DOCUMENT_19 FLAG_DOCUMENT_20 FLAG_DOCUMENT_21  \\\n",
              "0  ...                 0                0                0                0   \n",
              "1  ...                 0                0                0                0   \n",
              "2  ...                 0                0                0                0   \n",
              "3  ...                 0                0                0                0   \n",
              "4  ...                 0                0                0                0   \n",
              "\n",
              "  AMT_REQ_CREDIT_BUREAU_HOUR AMT_REQ_CREDIT_BUREAU_DAY  \\\n",
              "0                        0.0                       0.0   \n",
              "1                        0.0                       0.0   \n",
              "2                        0.0                       0.0   \n",
              "3                        0.0                       0.0   \n",
              "4                        0.0                       0.0   \n",
              "\n",
              "   AMT_REQ_CREDIT_BUREAU_WEEK  AMT_REQ_CREDIT_BUREAU_MON  \\\n",
              "0                         0.0                        0.0   \n",
              "1                         0.0                        0.0   \n",
              "2                         0.0                        0.0   \n",
              "3                         0.0                        0.0   \n",
              "4                         0.0                        2.0   \n",
              "\n",
              "   AMT_REQ_CREDIT_BUREAU_QRT  AMT_REQ_CREDIT_BUREAU_YEAR  \n",
              "0                        0.0                         1.0  \n",
              "1                        0.0                         0.0  \n",
              "2                        0.0                         1.0  \n",
              "3                        0.0                         4.0  \n",
              "4                        0.0                         0.0  \n",
              "\n",
              "[5 rows x 122 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3c6dd11c-608f-4a5b-8d01-e8a5891eb878\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SK_ID_CURR</th>\n",
              "      <th>TARGET</th>\n",
              "      <th>NAME_CONTRACT_TYPE</th>\n",
              "      <th>CODE_GENDER</th>\n",
              "      <th>FLAG_OWN_CAR</th>\n",
              "      <th>FLAG_OWN_REALTY</th>\n",
              "      <th>CNT_CHILDREN</th>\n",
              "      <th>AMT_INCOME_TOTAL</th>\n",
              "      <th>AMT_CREDIT</th>\n",
              "      <th>AMT_ANNUITY</th>\n",
              "      <th>...</th>\n",
              "      <th>FLAG_DOCUMENT_18</th>\n",
              "      <th>FLAG_DOCUMENT_19</th>\n",
              "      <th>FLAG_DOCUMENT_20</th>\n",
              "      <th>FLAG_DOCUMENT_21</th>\n",
              "      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n",
              "      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n",
              "      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n",
              "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
              "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
              "      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>313802</td>\n",
              "      <td>0</td>\n",
              "      <td>Cash loans</td>\n",
              "      <td>M</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>270000.0</td>\n",
              "      <td>327024.0</td>\n",
              "      <td>15372.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>319656</td>\n",
              "      <td>0</td>\n",
              "      <td>Cash loans</td>\n",
              "      <td>F</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>0</td>\n",
              "      <td>108000.0</td>\n",
              "      <td>675000.0</td>\n",
              "      <td>19737.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>207678</td>\n",
              "      <td>0</td>\n",
              "      <td>Revolving loans</td>\n",
              "      <td>F</td>\n",
              "      <td>Y</td>\n",
              "      <td>Y</td>\n",
              "      <td>2</td>\n",
              "      <td>112500.0</td>\n",
              "      <td>270000.0</td>\n",
              "      <td>13500.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>381593</td>\n",
              "      <td>0</td>\n",
              "      <td>Cash loans</td>\n",
              "      <td>F</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "      <td>67500.0</td>\n",
              "      <td>142200.0</td>\n",
              "      <td>9630.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>258153</td>\n",
              "      <td>0</td>\n",
              "      <td>Cash loans</td>\n",
              "      <td>F</td>\n",
              "      <td>Y</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>337500.0</td>\n",
              "      <td>1483231.5</td>\n",
              "      <td>46570.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 122 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c6dd11c-608f-4a5b-8d01-e8a5891eb878')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3c6dd11c-608f-4a5b-8d01-e8a5891eb878 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3c6dd11c-608f-4a5b-8d01-e8a5891eb878');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8e9ca52f-e3e8-479e-bead-ccfd6539ce1b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8e9ca52f-e3e8-479e-bead-ccfd6539ce1b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8e9ca52f-e3e8-479e-bead-ccfd6539ce1b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['TARGET'].value_counts(normalize=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJuPF2Wjnzu1",
        "outputId": "7ad3ce92-a833-413f-b1fc-308629c11557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.9201\n",
              "1    0.0799\n",
              "Name: TARGET, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93qit5TFasUz",
        "outputId": "6c0ef941-f885-4b0b-ce1e-c413a9f993c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Columns: 122 entries, SK_ID_CURR to AMT_REQ_CREDIT_BUREAU_YEAR\n",
            "dtypes: float64(65), int64(41), object(16)\n",
            "memory usage: 9.3+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = train_test_split(data,\n",
        "                                         test_size=TEST_SIZE,\n",
        "                                         stratify=data[TARGET_NAME],\n",
        "                                         random_state=RANDOM_STATE)"
      ],
      "metadata": {
        "id": "JD7eW8URasXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape, test_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muIIztwjn3S2",
        "outputId": "8894ad8e-f683-44a7-f761-5d85c612096b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8000, 122), (2000, 122))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline #1"
      ],
      "metadata": {
        "id": "GfudURdxn3VE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lightautoml.automl.base import AutoML\n",
        "from lightautoml.ml_algo.boost_lgbm import BoostLGBM\n",
        "from lightautoml.ml_algo.boost_cb import BoostCB\n",
        "from lightautoml.ml_algo.tuning.optuna import OptunaTuner\n",
        "from lightautoml.pipelines.features.lgb_pipeline import LGBSimpleFeatures\n",
        "from lightautoml.pipelines.ml.base import MLPipeline\n",
        "from lightautoml.pipelines.selection.importance_based import ImportanceCutoffSelector, ModelBasedImportanceEstimator\n",
        "from lightautoml.reader.base import PandasToPandasReader\n",
        "from lightautoml.tasks import Task\n",
        "from lightautoml.automl.blend import WeightedBlender"
      ],
      "metadata": {
        "id": "Vad3LgZ4okgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = Task('binary')\n",
        "reader = PandasToPandasReader(task, cv=N_FOLDS, random_state=RANDOM_STATE)"
      ],
      "metadata": {
        "id": "EjqFnxeioEpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe0 = LGBSimpleFeatures()\n",
        "mbie = ModelBasedImportanceEstimator()\n",
        "model0_lvl1 = BoostLGBM(\n",
        "    default_params={'learning_rate':0.05, 'num_leaves': 128, 'seed': RANDOM_STATE, 'num_threads': N_THREADS}\n",
        ")\n",
        "selector_lvl1 = ImportanceCutoffSelector(pipe0, model0, mbie, cutoff=0)\n",
        "\n",
        "\n",
        "pipe1 = LGBSimpleFeatures()\n",
        "params_tuner1_lvl1 = OptunaTuner(n_trials=20, timeout=30)\n",
        "model1_lvl1 = BoostLGBM(\n",
        "    default_params={'learning_rate':0.05, 'num_leaves': 128, 'seed': RANDOM_STATE, 'num_threads': N_THREADS}\n",
        ")\n",
        "model2_lvl1 = BoostLGBM(\n",
        "    default_params={'learning_rate':0.02, 'num_leaves': 64, 'seed': RANDOM_STATE, 'num_threads': N_THREADS}\n",
        ")\n",
        "\n",
        "\n",
        "#Pipeline - level 1\n",
        "pipeline_lvl1 = MLPipeline([\n",
        "    model0_lvl1,\n",
        "    (model1_lvl1, params_tuner1_lvl1),\n",
        "    model2_lvl1\n",
        "], pre_selection=selector_lvl1, features_pipeline=pipe1, post_selection=None)\n",
        "\n",
        "\n",
        "\n",
        "pipe2 = LGBSimpleFeatures()\n",
        "model_lvl2 = BoostLGBM(\n",
        "    default_params={'learning_rate':0.05, 'num_leaves': 128, 'seed': RANDOM_STATE, 'num_threads': N_THREADS}\n",
        ")\n",
        "\n",
        "pipeline_lvl2 = MLPipeline([model_lvl2], pre_selection=None, features_pipeline=pipe2, post_selection=None)\n",
        "\n",
        "\n",
        "automl = AutoML(reader,\n",
        "                [[pipeline_lvl1],\n",
        "                [pipeline_lvl2]],\n",
        "                skip_conn=False)\n",
        "\n",
        "oof_pred = automl.fit_predict(train_data, roles={'target': TARGET_NAME})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJa3lyIVoEr4",
        "outputId": "2bb735fa-2b62-47c3-af87-0215b232f429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.reader.base:\u001b[1mTrain data shape: (8000, 122)\u001b[0m\n",
            "\n",
            "INFO3:lightautoml.reader.base:Feats was rejected during automatic roles guess: ['REG_REGION_NOT_LIVE_REGION', 'LIVE_REGION_NOT_WORK_REGION', 'FLAG_DOCUMENT_8']\n",
            "INFO:lightautoml.automl.base:Layer \u001b[1m1\u001b[0m train process start. Time left 9999999982.76 secs\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.728681\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.726949\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[134]\tvalid's auc: 0.733604\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mMod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:lightautoml.ml_algo.base:Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m ...\n",
            "DEBUG:lightautoml.ml_algo.base:Training params: {'task': 'train', 'learning_rate': 0.05, 'num_leaves': 128, 'feature_fraction': 0.7, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'max_depth': -1, 'verbosity': -1, 'reg_alpha': 1, 'reg_lambda': 0.0, 'min_split_gain': 0.0, 'zero_as_missing': False, 'num_threads': 8, 'max_bin': 255, 'min_data_in_bin': 3, 'num_trees': 3000, 'early_stopping_rounds': 100, 'random_state': 42, 'seed': 42}\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.724367\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.729482\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[154]\tvalid's auc: 0.732406\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.720146\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.721096\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[114]\tvalid's auc: 0.724567\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.711012\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[77]\tvalid's auc: 0.716176\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.713851\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.706559\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[104]\tvalid's auc: 0.714196\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.733648\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[68]\tvalid's auc: 0.736169\n",
            "INFO:lightautoml.ml_algo.base:Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.7200040649032384\u001b[0m\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:lightautoml.ml_algo.tuning.optuna:Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m ... Time budget is 30.00 secs\n",
            "INFO:optuna.storages._in_memory:A new study created in memory with name: no-name-1558d87c-58ef-4863-92b8-f3b02c8427c6\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.732225\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.732727\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[136]\tvalid's auc: 0.73648\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 0 finished with value: 0.7364797322941558 and parameters: {'feature_fraction': 0.6872700594236812, 'num_leaves': 244}. Best is trial 0 with value: 0.7364797322941558.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 1\u001b[0m with hyperparameters {'feature_fraction': 0.6872700594236812, 'num_leaves': 244} scored 0.7364797322941558 in 0:00:08.267501\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.721779\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[89]\tvalid's auc: 0.725393\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 1 finished with value: 0.7253930325919036 and parameters: {'feature_fraction': 0.8659969709057025, 'num_leaves': 159}. Best is trial 0 with value: 0.7364797322941558.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 2\u001b[0m with hyperparameters {'feature_fraction': 0.8659969709057025, 'num_leaves': 159} scored 0.7253930325919036 in 0:00:06.647326\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.746182\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[98]\tvalid's auc: 0.749624\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 2 finished with value: 0.749624474130143 and parameters: {'feature_fraction': 0.5780093202212182, 'num_leaves': 53}. Best is trial 2 with value: 0.749624474130143.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 3\u001b[0m with hyperparameters {'feature_fraction': 0.5780093202212182, 'num_leaves': 53} scored 0.749624474130143 in 0:00:05.664822\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.745065\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.742199\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[105]\tvalid's auc: 0.745551\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 3 finished with value: 0.7455511543745423 and parameters: {'feature_fraction': 0.5290418060840998, 'num_leaves': 223}. Best is trial 2 with value: 0.749624474130143.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 4\u001b[0m with hyperparameters {'feature_fraction': 0.5290418060840998, 'num_leaves': 223} scored 0.7455511543745423 in 0:00:08.339460\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.720705\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.725142\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[174]\tvalid's auc: 0.7269\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 4 finished with value: 0.726900481635315 and parameters: {'feature_fraction': 0.8005575058716043, 'num_leaves': 185}. Best is trial 2 with value: 0.749624474130143.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 5\u001b[0m with hyperparameters {'feature_fraction': 0.8005575058716043, 'num_leaves': 185} scored 0.726900481635315 in 0:00:09.204657\n",
            "INFO:lightautoml.ml_algo.tuning.optuna:Hyperparameters optimization for \u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m completed\n",
            "INFO2:lightautoml.ml_algo.tuning.optuna:The set of hyperparameters \u001b[1m{'feature_fraction': 0.5780093202212182, 'num_leaves': 53}\u001b[0m\n",
            " achieve 0.7496 auc\n",
            "INFO:lightautoml.ml_algo.base:Start fitting \u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m ...\n",
            "DEBUG:lightautoml.ml_algo.base:Training params: {'task': 'train', 'learning_rate': 0.05, 'num_leaves': 53, 'feature_fraction': 0.5780093202212182, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'max_depth': -1, 'verbosity': -1, 'reg_alpha': 1, 'reg_lambda': 0.0, 'min_split_gain': 0.0, 'zero_as_missing': False, 'num_threads': 8, 'max_bin': 255, 'min_data_in_bin': 3, 'num_trees': 3000, 'early_stopping_rounds': 100, 'random_state': 42, 'seed': 42}\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.746182\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[98]\tvalid's auc: 0.749624\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.724243\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.725777\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[132]\tvalid's auc: 0.727045\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.706707\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[34]\tvalid's auc: 0.713528\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.72575\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[76]\tvalid's auc: 0.729545\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.73334\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[79]\tvalid's auc: 0.739905\n",
            "INFO:lightautoml.ml_algo.base:Fitting \u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m finished. score = \u001b[1m0.7271988585955801\u001b[0m\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:lightautoml.ml_algo.base:Start fitting \u001b[1mLvl_0_Pipe_0_Mod_2_LightGBM\u001b[0m ...\n",
            "DEBUG:lightautoml.ml_algo.base:Training params: {'task': 'train', 'learning_rate': 0.02, 'num_leaves': 64, 'feature_fraction': 0.7, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'max_depth': -1, 'verbosity': -1, 'reg_alpha': 1, 'reg_lambda': 0.0, 'min_split_gain': 0.0, 'zero_as_missing': False, 'num_threads': 8, 'max_bin': 255, 'min_data_in_bin': 3, 'num_trees': 3000, 'early_stopping_rounds': 100, 'random_state': 42, 'seed': 42}\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_2_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.744092\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.743739\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[125]\tvalid's auc: 0.745546\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_2_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.725528\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[65]\tvalid's auc: 0.73316\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_2_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.725071\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[16]\tvalid's auc: 0.733534\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_2_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.721122\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.727104\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[300]\tvalid's auc: 0.726998\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[216]\tvalid's auc: 0.728309\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_2_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.742416\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[78]\tvalid's auc: 0.744963\n",
            "INFO:lightautoml.ml_algo.base:Fitting \u001b[1mLvl_0_Pipe_0_Mod_2_LightGBM\u001b[0m finished. score = \u001b[1m0.7240795981188343\u001b[0m\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_0_Pipe_0_Mod_2_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:lightautoml.automl.base:Time left 9999999844.60 secs\n",
            "\n",
            "INFO:lightautoml.automl.base:\u001b[1mLayer 1 training completed.\u001b[0m\n",
            "\n",
            "INFO:lightautoml.automl.base:Layer \u001b[1m2\u001b[0m train process start. Time left 9999999844.60 secs\n",
            "INFO:lightautoml.ml_algo.base:Start fitting \u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m ...\n",
            "DEBUG:lightautoml.ml_algo.base:Training params: {'task': 'train', 'learning_rate': 0.05, 'num_leaves': 128, 'feature_fraction': 0.7, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'max_depth': -1, 'verbosity': -1, 'reg_alpha': 1, 'reg_lambda': 0.0, 'min_split_gain': 0.0, 'zero_as_missing': False, 'num_threads': 8, 'max_bin': 255, 'min_data_in_bin': 3, 'num_trees': 3000, 'early_stopping_rounds': 100, 'random_state': 42, 'seed': 42}\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.687156\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[16]\tvalid's auc: 0.724276\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.658394\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[16]\tvalid's auc: 0.691611\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.643669\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[2]\tvalid's auc: 0.686845\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.706559\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[42]\tvalid's auc: 0.72393\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.695864\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[12]\tvalid's auc: 0.724137\n",
            "INFO:lightautoml.ml_algo.base:Fitting \u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.6902522472303063\u001b[0m\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:lightautoml.automl.base:Time left 9999999833.59 secs\n",
            "\n",
            "INFO:lightautoml.automl.base:\u001b[1mLayer 2 training completed.\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = automl.predict(test_data)\n",
        "print('Prediction for test data:\\n{}\\nShape = {}'\n",
        "              .format(test_pred, test_pred.shape))\n",
        "\n",
        "print('Check scores...')\n",
        "print('OOF score: {}'.format(roc_auc_score(train_data[TARGET_NAME].values, oof_pred.data[:, 0])))\n",
        "print('TEST score: {}'.format(roc_auc_score(test_data[TARGET_NAME].values, test_pred.data[:, 0])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNJU5JXao7Pc",
        "outputId": "ade21253-280f-4ee1-9217-d5afd0c10c11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for test data:\n",
            "array([[0.05845162],\n",
            "       [0.08084685],\n",
            "       [0.07027299],\n",
            "       ...,\n",
            "       [0.08002303],\n",
            "       [0.09411833],\n",
            "       [0.14898382]], dtype=float32)\n",
            "Shape = (2000, 1)\n",
            "Check scores...\n",
            "OOF score: 0.6902522472303063\n",
            "TEST score: 0.7206589673913044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "automl.levels[0][0].ml_algos[0].get_features_score()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah6tre5juw89",
        "outputId": "5959a9e3-dfa1-4966-a38a-09243216f1f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EXT_SOURCE_3                  1885.253801\n",
              "EXT_SOURCE_2                  1785.728023\n",
              "DAYS_BIRTH                    1130.155983\n",
              "DAYS_REGISTRATION             1016.024376\n",
              "DAYS_ID_PUBLISH                961.878135\n",
              "                                 ...     \n",
              "REG_REGION_NOT_WORK_REGION       6.302472\n",
              "ord__NAME_CONTRACT_TYPE          5.599849\n",
              "AMT_REQ_CREDIT_BUREAU_HOUR       4.337980\n",
              "ord__EMERGENCYSTATE_MODE         2.951914\n",
              "FLAG_DOCUMENT_6                  1.502534\n",
              "Length: 96, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze fitted model\n",
        "print('Feature importances of selector:\\n{}'\n",
        "              .format(selector.get_features_score()))\n",
        "print('=' * 70)\n",
        "\n",
        "print('Feature importances of top level algorithm:\\n{}'\n",
        "              .format(automl.levels[-1][0].ml_algos[0].get_features_score()))\n",
        "print('=' * 70)\n",
        "\n",
        "print('Feature importances of lowest level algorithm - model 0:\\n{}'\n",
        "              .format(automl.levels[0][0].ml_algos[0].get_features_score()))\n",
        "print('=' * 70)\n",
        "\n",
        "print('Feature importances of lowest level algorithm - model 1:\\n{}'\n",
        "              .format(automl.levels[0][0].ml_algos[1].get_features_score()))\n",
        "print('=' * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OkikPuLo7Sx",
        "outputId": "799ae063-c0a6-43fc-fc5a-024b9262da97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature importances of selector:\n",
            "None\n",
            "======================================================================\n",
            "Feature importances of top level algorithm:\n",
            "Lvl_0_Pipe_0_Mod_2_LightGBM_prediction_0    2278.610948\n",
            "Lvl_0_Pipe_0_Mod_0_LightGBM_prediction_0    1189.739746\n",
            "Lvl_0_Pipe_0_Mod_1_LightGBM_prediction_0       0.000000\n",
            "dtype: float64\n",
            "======================================================================\n",
            "Feature importances of lowest level algorithm - model 0:\n",
            "EXT_SOURCE_3                  1885.253801\n",
            "EXT_SOURCE_2                  1785.728023\n",
            "DAYS_BIRTH                    1130.155983\n",
            "DAYS_REGISTRATION             1016.024376\n",
            "DAYS_ID_PUBLISH                961.878135\n",
            "                                 ...     \n",
            "REG_REGION_NOT_WORK_REGION       6.302472\n",
            "ord__NAME_CONTRACT_TYPE          5.599849\n",
            "AMT_REQ_CREDIT_BUREAU_HOUR       4.337980\n",
            "ord__EMERGENCYSTATE_MODE         2.951914\n",
            "FLAG_DOCUMENT_6                  1.502534\n",
            "Length: 96, dtype: float64\n",
            "======================================================================\n",
            "Feature importances of lowest level algorithm - model 1:\n",
            "EXT_SOURCE_3                  1657.010943\n",
            "EXT_SOURCE_2                  1549.253864\n",
            "DAYS_BIRTH                     987.023480\n",
            "DAYS_ID_PUBLISH                762.362100\n",
            "SK_ID_CURR                     755.458491\n",
            "                                 ...     \n",
            "ord__NAME_CONTRACT_TYPE          6.856702\n",
            "AMT_REQ_CREDIT_BUREAU_HOUR       5.509874\n",
            "REG_REGION_NOT_WORK_REGION       4.249416\n",
            "ord__EMERGENCYSTATE_MODE         3.456580\n",
            "FLAG_DOCUMENT_6                  0.000000\n",
            "Length: 96, dtype: float64\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline - v2 (+Optuna last level)"
      ],
      "metadata": {
        "id": "9FwWrwhYutlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = Task('binary')\n",
        "reader = PandasToPandasReader(task, cv=N_FOLDS, random_state=RANDOM_STATE)\n",
        "\n",
        "\n",
        "\n",
        "pipe0 = LGBSimpleFeatures()\n",
        "mbie = ModelBasedImportanceEstimator()\n",
        "model0_lvl1 = BoostLGBM(\n",
        "    default_params={'learning_rate':0.05, 'num_leaves': 128, 'seed': RANDOM_STATE, 'num_threads': N_THREADS}\n",
        ")\n",
        "selector_lvl1 = ImportanceCutoffSelector(pipe0, model0, mbie, cutoff=0)\n",
        "\n",
        "\n",
        "pipe1 = LGBSimpleFeatures()\n",
        "params_tuner1_lvl1 = OptunaTuner(n_trials=20, timeout=30)\n",
        "model1_lvl1 = BoostLGBM(\n",
        "    default_params={'learning_rate':0.05, 'num_leaves': 128, 'seed': RANDOM_STATE, 'num_threads': N_THREADS}\n",
        ")\n",
        "model2_lvl1 = BoostLGBM(\n",
        "    default_params={'learning_rate':0.02, 'num_leaves': 64, 'seed': RANDOM_STATE, 'num_threads': N_THREADS}\n",
        ")\n",
        "\n",
        "\n",
        "#Pipeline - level 1\n",
        "pipeline_lvl1 = MLPipeline([\n",
        "    model0_lvl1,\n",
        "    (model1_lvl1, params_tuner1_lvl1),\n",
        "    model2_lvl1\n",
        "], pre_selection=selector_lvl1, features_pipeline=pipe1, post_selection=None)\n",
        "\n",
        "\n",
        "\n",
        "pipe2 = LGBSimpleFeatures()\n",
        "model_lvl2 = BoostLGBM(\n",
        "    default_params={'learning_rate':0.05, 'num_leaves': 128, 'seed': RANDOM_STATE, 'num_threads': N_THREADS}\n",
        ")\n",
        "params_tuner2_lvl2 = OptunaTuner(n_trials=20, timeout=30)\n",
        "pipeline_lvl2 = MLPipeline([(model_lvl2, params_tuner2_lvl2)], pre_selection=None, features_pipeline=pipe2, post_selection=None)\n",
        "\n",
        "\n",
        "automl = AutoML(reader,\n",
        "                [[pipeline_lvl1],\n",
        "                [pipeline_lvl2]],\n",
        "                skip_conn=False)\n",
        "\n",
        "oof_pred = automl.fit_predict(train_data, roles={'target': TARGET_NAME})\n",
        "\n",
        "\n",
        "test_pred = automl.predict(test_data)\n",
        "print('Prediction for test data:\\n{}\\nShape = {}'\n",
        "              .format(test_pred, test_pred.shape))\n",
        "\n",
        "print('Check scores...')\n",
        "print('OOF score: {}'.format(roc_auc_score(train_data[TARGET_NAME].values, oof_pred.data[:, 0])))\n",
        "print('TEST score: {}'.format(roc_auc_score(test_data[TARGET_NAME].values, test_pred.data[:, 0])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCBlRRWcutqk",
        "outputId": "c1d3a432-6f90-45e9-97df-6f1fff64ee3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.reader.base:\u001b[1mTrain data shape: (8000, 122)\u001b[0m\n",
            "\n",
            "INFO3:lightautoml.reader.base:Feats was rejected during automatic roles guess: ['REG_REGION_NOT_LIVE_REGION', 'LIVE_REGION_NOT_WORK_REGION', 'FLAG_DOCUMENT_8']\n",
            "INFO:lightautoml.automl.base:Layer \u001b[1m1\u001b[0m train process start. Time left 9999999984.67 secs\n",
            "INFO:lightautoml.ml_algo.base:Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m ...\n",
            "DEBUG:lightautoml.ml_algo.base:Training params: {'task': 'train', 'learning_rate': 0.05, 'num_leaves': 128, 'feature_fraction': 0.7, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'max_depth': -1, 'verbosity': -1, 'reg_alpha': 1, 'reg_lambda': 0.0, 'min_split_gain': 0.0, 'zero_as_missing': False, 'num_threads': 8, 'max_bin': 255, 'min_data_in_bin': 3, 'num_trees': 3000, 'early_stopping_rounds': 100, 'random_state': 42, 'seed': 42}\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.724367\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.729482\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[154]\tvalid's auc: 0.732406\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.720146\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.721096\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[114]\tvalid's auc: 0.724567\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.711012\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[77]\tvalid's auc: 0.716176\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.713851\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.706559\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[104]\tvalid's auc: 0.714196\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.733648\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[68]\tvalid's auc: 0.736169\n",
            "INFO:lightautoml.ml_algo.base:Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.7200040649032384\u001b[0m\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:lightautoml.ml_algo.tuning.optuna:Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m ... Time budget is 30.00 secs\n",
            "INFO:optuna.storages._in_memory:A new study created in memory with name: no-name-806eaaa3-c438-4829-9aa5-4ef664ef44e7\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.732225\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.732727\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[136]\tvalid's auc: 0.73648\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 0 finished with value: 0.7364797322941558 and parameters: {'feature_fraction': 0.6872700594236812, 'num_leaves': 244}. Best is trial 0 with value: 0.7364797322941558.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 1\u001b[0m with hyperparameters {'feature_fraction': 0.6872700594236812, 'num_leaves': 244} scored 0.7364797322941558 in 0:00:09.618738\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.721779\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[89]\tvalid's auc: 0.725393\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 1 finished with value: 0.7253930325919036 and parameters: {'feature_fraction': 0.8659969709057025, 'num_leaves': 159}. Best is trial 0 with value: 0.7364797322941558.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 2\u001b[0m with hyperparameters {'feature_fraction': 0.8659969709057025, 'num_leaves': 159} scored 0.7253930325919036 in 0:00:06.996596\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.746182\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[98]\tvalid's auc: 0.749624\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 2 finished with value: 0.749624474130143 and parameters: {'feature_fraction': 0.5780093202212182, 'num_leaves': 53}. Best is trial 2 with value: 0.749624474130143.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 3\u001b[0m with hyperparameters {'feature_fraction': 0.5780093202212182, 'num_leaves': 53} scored 0.749624474130143 in 0:00:05.658456\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.745065\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.742199\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[105]\tvalid's auc: 0.745551\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 3 finished with value: 0.7455511543745423 and parameters: {'feature_fraction': 0.5290418060840998, 'num_leaves': 223}. Best is trial 2 with value: 0.749624474130143.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 4\u001b[0m with hyperparameters {'feature_fraction': 0.5290418060840998, 'num_leaves': 223} scored 0.7455511543745423 in 0:00:08.409824\n",
            "INFO:lightautoml.ml_algo.tuning.optuna:Hyperparameters optimization for \u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m completed\n",
            "INFO2:lightautoml.ml_algo.tuning.optuna:The set of hyperparameters \u001b[1m{'feature_fraction': 0.5780093202212182, 'num_leaves': 53}\u001b[0m\n",
            " achieve 0.7496 auc\n",
            "INFO:lightautoml.ml_algo.base:Start fitting \u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m ...\n",
            "DEBUG:lightautoml.ml_algo.base:Training params: {'task': 'train', 'learning_rate': 0.05, 'num_leaves': 53, 'feature_fraction': 0.5780093202212182, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'max_depth': -1, 'verbosity': -1, 'reg_alpha': 1, 'reg_lambda': 0.0, 'min_split_gain': 0.0, 'zero_as_missing': False, 'num_threads': 8, 'max_bin': 255, 'min_data_in_bin': 3, 'num_trees': 3000, 'early_stopping_rounds': 100, 'random_state': 42, 'seed': 42}\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.746182\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[98]\tvalid's auc: 0.749624\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.724243\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.725777\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[132]\tvalid's auc: 0.727045\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.706707\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[34]\tvalid's auc: 0.713528\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.72575\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[76]\tvalid's auc: 0.729545\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.73334\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[79]\tvalid's auc: 0.739905\n",
            "INFO:lightautoml.ml_algo.base:Fitting \u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m finished. score = \u001b[1m0.7271988585955801\u001b[0m\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:lightautoml.ml_algo.base:Start fitting \u001b[1mLvl_0_Pipe_0_Mod_2_LightGBM\u001b[0m ...\n",
            "DEBUG:lightautoml.ml_algo.base:Training params: {'task': 'train', 'learning_rate': 0.02, 'num_leaves': 64, 'feature_fraction': 0.7, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'max_depth': -1, 'verbosity': -1, 'reg_alpha': 1, 'reg_lambda': 0.0, 'min_split_gain': 0.0, 'zero_as_missing': False, 'num_threads': 8, 'max_bin': 255, 'min_data_in_bin': 3, 'num_trees': 3000, 'early_stopping_rounds': 100, 'random_state': 42, 'seed': 42}\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_2_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.744092\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.743739\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[125]\tvalid's auc: 0.745546\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_2_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.725528\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[65]\tvalid's auc: 0.73316\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_2_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.725071\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[16]\tvalid's auc: 0.733534\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_2_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.721122\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.727104\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[300]\tvalid's auc: 0.726998\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[216]\tvalid's auc: 0.728309\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_2_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.742416\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[78]\tvalid's auc: 0.744963\n",
            "INFO:lightautoml.ml_algo.base:Fitting \u001b[1mLvl_0_Pipe_0_Mod_2_LightGBM\u001b[0m finished. score = \u001b[1m0.7240795981188343\u001b[0m\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_0_Pipe_0_Mod_2_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:lightautoml.automl.base:Time left 9999999857.79 secs\n",
            "\n",
            "INFO:lightautoml.automl.base:\u001b[1mLayer 1 training completed.\u001b[0m\n",
            "\n",
            "INFO:lightautoml.automl.base:Layer \u001b[1m2\u001b[0m train process start. Time left 9999999857.78 secs\n",
            "INFO:lightautoml.ml_algo.tuning.optuna:Start hyperparameters optimization for \u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m ... Time budget is 30.00 secs\n",
            "INFO:optuna.storages._in_memory:A new study created in memory with name: no-name-39bae87a-ce93-4950-b37c-5bab90628c57\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.687156\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[16]\tvalid's auc: 0.724276\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 0 finished with value: 0.7242758097193045 and parameters: {'feature_fraction': 0.6872700594236812, 'num_leaves': 244}. Best is trial 0 with value: 0.7242758097193045.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 1\u001b[0m with hyperparameters {'feature_fraction': 0.6872700594236812, 'num_leaves': 244} scored 0.7242758097193045 in 0:00:03.398628\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.692558\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[14]\tvalid's auc: 0.734545\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 1 finished with value: 0.7345446381320461 and parameters: {'feature_fraction': 0.8659969709057025, 'num_leaves': 159}. Best is trial 1 with value: 0.7345446381320461.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 2\u001b[0m with hyperparameters {'feature_fraction': 0.8659969709057025, 'num_leaves': 159} scored 0.7345446381320461 in 0:00:03.290627\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.699708\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[14]\tvalid's auc: 0.731137\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 2 finished with value: 0.7311368410924195 and parameters: {'feature_fraction': 0.5780093202212182, 'num_leaves': 53}. Best is trial 1 with value: 0.7345446381320461.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 3\u001b[0m with hyperparameters {'feature_fraction': 0.5780093202212182, 'num_leaves': 53} scored 0.7311368410924195 in 0:00:01.739816\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.687156\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[16]\tvalid's auc: 0.724276\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 3 finished with value: 0.7242758097193045 and parameters: {'feature_fraction': 0.5290418060840998, 'num_leaves': 223}. Best is trial 1 with value: 0.7345446381320461.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 4\u001b[0m with hyperparameters {'feature_fraction': 0.5290418060840998, 'num_leaves': 223} scored 0.7242758097193045 in 0:00:02.395915\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.687156\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[16]\tvalid's auc: 0.724276\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 4 finished with value: 0.7242758097193045 and parameters: {'feature_fraction': 0.8005575058716043, 'num_leaves': 185}. Best is trial 1 with value: 0.7345446381320461.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 5\u001b[0m with hyperparameters {'feature_fraction': 0.8005575058716043, 'num_leaves': 185} scored 0.7242758097193045 in 0:00:02.561996\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.687156\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[16]\tvalid's auc: 0.724276\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 5 finished with value: 0.7242758097193045 and parameters: {'feature_fraction': 0.5102922471479012, 'num_leaves': 248}. Best is trial 1 with value: 0.7345446381320461.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 6\u001b[0m with hyperparameters {'feature_fraction': 0.5102922471479012, 'num_leaves': 248} scored 0.7242758097193045 in 0:00:04.167611\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.696864\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[14]\tvalid's auc: 0.733957\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 6 finished with value: 0.7339566260938359 and parameters: {'feature_fraction': 0.9162213204002109, 'num_leaves': 66}. Best is trial 1 with value: 0.7345446381320461.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 7\u001b[0m with hyperparameters {'feature_fraction': 0.9162213204002109, 'num_leaves': 66} scored 0.7339566260938359 in 0:00:01.838260\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.695672\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[16]\tvalid's auc: 0.728133\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 7 finished with value: 0.7281326341335643 and parameters: {'feature_fraction': 0.5909124836035503, 'num_leaves': 60}. Best is trial 1 with value: 0.7345446381320461.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 8\u001b[0m with hyperparameters {'feature_fraction': 0.5909124836035503, 'num_leaves': 60} scored 0.7281326341335643 in 0:00:01.607659\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.687156\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[16]\tvalid's auc: 0.724276\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 8 finished with value: 0.7242758097193045 and parameters: {'feature_fraction': 0.6521211214797689, 'num_leaves': 141}. Best is trial 1 with value: 0.7345446381320461.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 9\u001b[0m with hyperparameters {'feature_fraction': 0.6521211214797689, 'num_leaves': 141} scored 0.7242758097193045 in 0:00:02.844441\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.692162\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[16]\tvalid's auc: 0.724319\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 9 finished with value: 0.7243185742311742 and parameters: {'feature_fraction': 0.7159725093210578, 'num_leaves': 85}. Best is trial 1 with value: 0.7345446381320461.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 10\u001b[0m with hyperparameters {'feature_fraction': 0.7159725093210578, 'num_leaves': 85} scored 0.7243185742311742 in 0:00:01.957262\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.692558\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[14]\tvalid's auc: 0.734545\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 10 finished with value: 0.7345446381320461 and parameters: {'feature_fraction': 0.9851459267858992, 'num_leaves': 129}. Best is trial 1 with value: 0.7345446381320461.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 11\u001b[0m with hyperparameters {'feature_fraction': 0.9851459267858992, 'num_leaves': 129} scored 0.7345446381320461 in 0:00:02.664923\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.692558\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[14]\tvalid's auc: 0.734545\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 11 finished with value: 0.7345446381320461 and parameters: {'feature_fraction': 0.9847685553939328, 'num_leaves': 138}. Best is trial 1 with value: 0.7345446381320461.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 12\u001b[0m with hyperparameters {'feature_fraction': 0.9847685553939328, 'num_leaves': 138} scored 0.7345446381320461 in 0:00:02.563966\n",
            "INFO:lightautoml.ml_algo.tuning.optuna:Hyperparameters optimization for \u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m completed\n",
            "INFO2:lightautoml.ml_algo.tuning.optuna:The set of hyperparameters \u001b[1m{'feature_fraction': 0.8659969709057025, 'num_leaves': 159}\u001b[0m\n",
            " achieve 0.7345 auc\n",
            "INFO:lightautoml.ml_algo.base:Start fitting \u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m ...\n",
            "DEBUG:lightautoml.ml_algo.base:Training params: {'task': 'train', 'learning_rate': 0.05, 'num_leaves': 159, 'feature_fraction': 0.8659969709057025, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'max_depth': -1, 'verbosity': -1, 'reg_alpha': 1, 'reg_lambda': 0.0, 'min_split_gain': 0.0, 'zero_as_missing': False, 'num_threads': 8, 'max_bin': 255, 'min_data_in_bin': 3, 'num_trees': 3000, 'early_stopping_rounds': 100, 'random_state': 42, 'seed': 42}\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.692558\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[14]\tvalid's auc: 0.734545\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.683193\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[13]\tvalid's auc: 0.723882\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.616553\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[30]\tvalid's auc: 0.650807\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.699757\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[12]\tvalid's auc: 0.721847\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.702363\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[10]\tvalid's auc: 0.742877\n",
            "INFO:lightautoml.ml_algo.base:Fitting \u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.7054033661735845\u001b[0m\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:lightautoml.automl.base:Time left 9999999814.53 secs\n",
            "\n",
            "INFO:lightautoml.automl.base:\u001b[1mLayer 2 training completed.\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for test data:\n",
            "array([[0.06539972],\n",
            "       [0.07553179],\n",
            "       [0.05480719],\n",
            "       ...,\n",
            "       [0.06585301],\n",
            "       [0.06425362],\n",
            "       [0.18560858]], dtype=float32)\n",
            "Shape = (2000, 1)\n",
            "Check scores...\n",
            "OOF score: 0.7054033661735845\n",
            "TEST score: 0.7167340353260869\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline - v3 + catboost model"
      ],
      "metadata": {
        "id": "p3KZxkkKvjOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = Task('binary')\n",
        "reader = PandasToPandasReader(task, cv=N_FOLDS, random_state=RANDOM_STATE)\n",
        "\n",
        "\n",
        "\n",
        "pipe0 = LGBSimpleFeatures()\n",
        "mbie = ModelBasedImportanceEstimator()\n",
        "model0_lvl1 = BoostLGBM(\n",
        "    default_params={'learning_rate':0.05, 'num_leaves': 128, 'seed': RANDOM_STATE, 'num_threads': N_THREADS}\n",
        ")\n",
        "selector_lvl1 = ImportanceCutoffSelector(pipe0, model0, mbie, cutoff=0)\n",
        "\n",
        "\n",
        "pipe1 = LGBSimpleFeatures()\n",
        "params_tuner1_lvl1 = OptunaTuner(n_trials=20, timeout=30)\n",
        "model1_lvl1 = BoostLGBM(\n",
        "    default_params={'learning_rate':0.05, 'num_leaves': 128, 'seed': RANDOM_STATE, 'num_threads': N_THREADS}\n",
        ")\n",
        "model2_lvl1 = BoostLGBM(\n",
        "    default_params={'learning_rate':0.02, 'num_leaves': 64, 'seed': RANDOM_STATE, 'num_threads': N_THREADS}\n",
        ")\n",
        "\n",
        "\n",
        "#Pipeline - level 1\n",
        "pipeline_lvl1 = MLPipeline([\n",
        "    model0_lvl1,\n",
        "    (model1_lvl1, params_tuner1_lvl1),\n",
        "    model2_lvl1\n",
        "], pre_selection=selector_lvl1, features_pipeline=pipe1, post_selection=None)\n",
        "\n",
        "\n",
        "\n",
        "pipe2 = LGBSimpleFeatures()\n",
        "model_lvl2 = BoostLGBM(\n",
        "    default_params={'learning_rate':0.05, 'num_leaves': 128, 'seed': RANDOM_STATE, 'num_threads': N_THREADS}\n",
        ")\n",
        "params_tuner2_lvl2 = OptunaTuner(n_trials=20, timeout=30)\n",
        "\n",
        "model_cb_lvl2 = BoostCB()\n",
        "\n",
        "pipeline_lvl2 = MLPipeline(\n",
        "    [(model_lvl2, params_tuner2_lvl2),\n",
        "     model_cb_lvl2],\n",
        "    pre_selection=None, features_pipeline=pipe2, post_selection=None)\n",
        "\n",
        "\n",
        "automl = AutoML(reader,\n",
        "                [[pipeline_lvl1],\n",
        "                [pipeline_lvl2]],\n",
        "                skip_conn=False)\n",
        "\n",
        "oof_pred = automl.fit_predict(train_data, roles={'target': TARGET_NAME})\n",
        "\n",
        "\n",
        "test_pred = automl.predict(test_data)\n",
        "print('Prediction for test data:\\n{}\\nShape = {}'\n",
        "              .format(test_pred, test_pred.shape))\n",
        "\n",
        "print('Check scores...')\n",
        "print('OOF score: {}'.format(roc_auc_score(train_data[TARGET_NAME].values, oof_pred.data[:, 0])))\n",
        "print('TEST score: {}'.format(roc_auc_score(test_data[TARGET_NAME].values, test_pred.data[:, 0])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmcSC46rvjRc",
        "outputId": "b1d8ed66-78c2-45c7-84a0-53e44ff4c914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.reader.base:\u001b[1mTrain data shape: (8000, 122)\u001b[0m\n",
            "\n",
            "INFO3:lightautoml.reader.base:Feats was rejected during automatic roles guess: ['REG_REGION_NOT_LIVE_REGION', 'LIVE_REGION_NOT_WORK_REGION', 'FLAG_DOCUMENT_8']\n",
            "INFO:lightautoml.automl.base:Layer \u001b[1m1\u001b[0m train process start. Time left 9999999997.74 secs\n",
            "INFO:lightautoml.ml_algo.base:Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m ...\n",
            "DEBUG:lightautoml.ml_algo.base:Training params: {'task': 'train', 'learning_rate': 0.05, 'num_leaves': 128, 'feature_fraction': 0.7, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'max_depth': -1, 'verbosity': -1, 'reg_alpha': 1, 'reg_lambda': 0.0, 'min_split_gain': 0.0, 'zero_as_missing': False, 'num_threads': 8, 'max_bin': 255, 'min_data_in_bin': 3, 'num_trees': 3000, 'early_stopping_rounds': 100, 'random_state': 42, 'seed': 42}\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.724367\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.729482\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[154]\tvalid's auc: 0.732406\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.720146\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.721096\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[114]\tvalid's auc: 0.724567\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.711012\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[77]\tvalid's auc: 0.716176\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.713851\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.706559\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[104]\tvalid's auc: 0.714196\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.733648\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[68]\tvalid's auc: 0.736169\n",
            "INFO:lightautoml.ml_algo.base:Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.7200040649032384\u001b[0m\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:lightautoml.ml_algo.tuning.optuna:Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m ... Time budget is 30.00 secs\n",
            "INFO:optuna.storages._in_memory:A new study created in memory with name: no-name-84fd5d2b-28d6-4513-8bbd-ee1f6d89e0d4\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.732225\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.732727\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[136]\tvalid's auc: 0.73648\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 0 finished with value: 0.7364797322941558 and parameters: {'feature_fraction': 0.6872700594236812, 'num_leaves': 244}. Best is trial 0 with value: 0.7364797322941558.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 1\u001b[0m with hyperparameters {'feature_fraction': 0.6872700594236812, 'num_leaves': 244} scored 0.7364797322941558 in 0:00:08.001735\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.721779\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[89]\tvalid's auc: 0.725393\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 1 finished with value: 0.7253930325919036 and parameters: {'feature_fraction': 0.8659969709057025, 'num_leaves': 159}. Best is trial 0 with value: 0.7364797322941558.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 2\u001b[0m with hyperparameters {'feature_fraction': 0.8659969709057025, 'num_leaves': 159} scored 0.7253930325919036 in 0:00:07.283664\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.746182\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[98]\tvalid's auc: 0.749624\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 2 finished with value: 0.749624474130143 and parameters: {'feature_fraction': 0.5780093202212182, 'num_leaves': 53}. Best is trial 2 with value: 0.749624474130143.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 3\u001b[0m with hyperparameters {'feature_fraction': 0.5780093202212182, 'num_leaves': 53} scored 0.749624474130143 in 0:00:05.459867\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.745065\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.742199\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[105]\tvalid's auc: 0.745551\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 3 finished with value: 0.7455511543745423 and parameters: {'feature_fraction': 0.5290418060840998, 'num_leaves': 223}. Best is trial 2 with value: 0.749624474130143.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 4\u001b[0m with hyperparameters {'feature_fraction': 0.5290418060840998, 'num_leaves': 223} scored 0.7455511543745423 in 0:00:06.915391\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.720705\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.725142\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[174]\tvalid's auc: 0.7269\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 4 finished with value: 0.726900481635315 and parameters: {'feature_fraction': 0.8005575058716043, 'num_leaves': 185}. Best is trial 2 with value: 0.749624474130143.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 5\u001b[0m with hyperparameters {'feature_fraction': 0.8005575058716043, 'num_leaves': 185} scored 0.726900481635315 in 0:00:09.454705\n",
            "INFO:lightautoml.ml_algo.tuning.optuna:Hyperparameters optimization for \u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m completed\n",
            "INFO2:lightautoml.ml_algo.tuning.optuna:The set of hyperparameters \u001b[1m{'feature_fraction': 0.5780093202212182, 'num_leaves': 53}\u001b[0m\n",
            " achieve 0.7496 auc\n",
            "INFO:lightautoml.ml_algo.base:Start fitting \u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m ...\n",
            "DEBUG:lightautoml.ml_algo.base:Training params: {'task': 'train', 'learning_rate': 0.05, 'num_leaves': 53, 'feature_fraction': 0.5780093202212182, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'max_depth': -1, 'verbosity': -1, 'reg_alpha': 1, 'reg_lambda': 0.0, 'min_split_gain': 0.0, 'zero_as_missing': False, 'num_threads': 8, 'max_bin': 255, 'min_data_in_bin': 3, 'num_trees': 3000, 'early_stopping_rounds': 100, 'random_state': 42, 'seed': 42}\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.746182\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[98]\tvalid's auc: 0.749624\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.724243\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.725777\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[132]\tvalid's auc: 0.727045\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.706707\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[34]\tvalid's auc: 0.713528\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.72575\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[76]\tvalid's auc: 0.729545\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.73334\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[79]\tvalid's auc: 0.739905\n",
            "INFO:lightautoml.ml_algo.base:Fitting \u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m finished. score = \u001b[1m0.7271988585955801\u001b[0m\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_0_Pipe_0_Mod_1_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:lightautoml.ml_algo.base:Start fitting \u001b[1mLvl_0_Pipe_0_Mod_2_LightGBM\u001b[0m ...\n",
            "DEBUG:lightautoml.ml_algo.base:Training params: {'task': 'train', 'learning_rate': 0.02, 'num_leaves': 64, 'feature_fraction': 0.7, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'max_depth': -1, 'verbosity': -1, 'reg_alpha': 1, 'reg_lambda': 0.0, 'min_split_gain': 0.0, 'zero_as_missing': False, 'num_threads': 8, 'max_bin': 255, 'min_data_in_bin': 3, 'num_trees': 3000, 'early_stopping_rounds': 100, 'random_state': 42, 'seed': 42}\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_2_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.744092\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.743739\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[125]\tvalid's auc: 0.745546\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_2_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.725528\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[65]\tvalid's auc: 0.73316\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_2_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.725071\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[16]\tvalid's auc: 0.733534\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_2_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.721122\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.727104\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[300]\tvalid's auc: 0.726998\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[216]\tvalid's auc: 0.728309\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_2_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.742416\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[78]\tvalid's auc: 0.744963\n",
            "INFO:lightautoml.ml_algo.base:Fitting \u001b[1mLvl_0_Pipe_0_Mod_2_LightGBM\u001b[0m finished. score = \u001b[1m0.7240795981188343\u001b[0m\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_0_Pipe_0_Mod_2_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:lightautoml.automl.base:Time left 9999999867.75 secs\n",
            "\n",
            "INFO:lightautoml.automl.base:\u001b[1mLayer 1 training completed.\u001b[0m\n",
            "\n",
            "INFO:lightautoml.automl.base:Layer \u001b[1m2\u001b[0m train process start. Time left 9999999867.75 secs\n",
            "INFO:lightautoml.ml_algo.tuning.optuna:Start hyperparameters optimization for \u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m ... Time budget is 30.00 secs\n",
            "INFO:optuna.storages._in_memory:A new study created in memory with name: no-name-6211ac3c-8b2e-4570-b27f-bdcbc01bdbad\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.687156\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[16]\tvalid's auc: 0.724276\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 0 finished with value: 0.7242758097193045 and parameters: {'feature_fraction': 0.6872700594236812, 'num_leaves': 244}. Best is trial 0 with value: 0.7242758097193045.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 1\u001b[0m with hyperparameters {'feature_fraction': 0.6872700594236812, 'num_leaves': 244} scored 0.7242758097193045 in 0:00:02.799862\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.692558\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[14]\tvalid's auc: 0.734545\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 1 finished with value: 0.7345446381320461 and parameters: {'feature_fraction': 0.8659969709057025, 'num_leaves': 159}. Best is trial 1 with value: 0.7345446381320461.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 2\u001b[0m with hyperparameters {'feature_fraction': 0.8659969709057025, 'num_leaves': 159} scored 0.7345446381320461 in 0:00:02.162649\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.699708\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[14]\tvalid's auc: 0.731137\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 2 finished with value: 0.7311368410924195 and parameters: {'feature_fraction': 0.5780093202212182, 'num_leaves': 53}. Best is trial 1 with value: 0.7345446381320461.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 3\u001b[0m with hyperparameters {'feature_fraction': 0.5780093202212182, 'num_leaves': 53} scored 0.7311368410924195 in 0:00:01.342043\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.687156\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[16]\tvalid's auc: 0.724276\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 3 finished with value: 0.7242758097193045 and parameters: {'feature_fraction': 0.5290418060840998, 'num_leaves': 223}. Best is trial 1 with value: 0.7345446381320461.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 4\u001b[0m with hyperparameters {'feature_fraction': 0.5290418060840998, 'num_leaves': 223} scored 0.7242758097193045 in 0:00:02.056503\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.687156\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[16]\tvalid's auc: 0.724276\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 4 finished with value: 0.7242758097193045 and parameters: {'feature_fraction': 0.8005575058716043, 'num_leaves': 185}. Best is trial 1 with value: 0.7345446381320461.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 5\u001b[0m with hyperparameters {'feature_fraction': 0.8005575058716043, 'num_leaves': 185} scored 0.7242758097193045 in 0:00:02.045162\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.687156\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[16]\tvalid's auc: 0.724276\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 5 finished with value: 0.7242758097193045 and parameters: {'feature_fraction': 0.5102922471479012, 'num_leaves': 248}. Best is trial 1 with value: 0.7345446381320461.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 6\u001b[0m with hyperparameters {'feature_fraction': 0.5102922471479012, 'num_leaves': 248} scored 0.7242758097193045 in 0:00:02.821390\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.696864\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[14]\tvalid's auc: 0.733957\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 6 finished with value: 0.7339566260938359 and parameters: {'feature_fraction': 0.9162213204002109, 'num_leaves': 66}. Best is trial 1 with value: 0.7345446381320461.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 7\u001b[0m with hyperparameters {'feature_fraction': 0.9162213204002109, 'num_leaves': 66} scored 0.7339566260938359 in 0:00:01.999151\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.695672\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[16]\tvalid's auc: 0.728133\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 7 finished with value: 0.7281326341335643 and parameters: {'feature_fraction': 0.5909124836035503, 'num_leaves': 60}. Best is trial 1 with value: 0.7345446381320461.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 8\u001b[0m with hyperparameters {'feature_fraction': 0.5909124836035503, 'num_leaves': 60} scored 0.7281326341335643 in 0:00:01.483734\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.687156\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[16]\tvalid's auc: 0.724276\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 8 finished with value: 0.7242758097193045 and parameters: {'feature_fraction': 0.6521211214797689, 'num_leaves': 141}. Best is trial 1 with value: 0.7345446381320461.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 9\u001b[0m with hyperparameters {'feature_fraction': 0.6521211214797689, 'num_leaves': 141} scored 0.7242758097193045 in 0:00:02.047728\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.692162\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[16]\tvalid's auc: 0.724319\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 9 finished with value: 0.7243185742311742 and parameters: {'feature_fraction': 0.7159725093210578, 'num_leaves': 85}. Best is trial 1 with value: 0.7345446381320461.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 10\u001b[0m with hyperparameters {'feature_fraction': 0.7159725093210578, 'num_leaves': 85} scored 0.7243185742311742 in 0:00:01.885637\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.692558\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[14]\tvalid's auc: 0.734545\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 10 finished with value: 0.7345446381320461 and parameters: {'feature_fraction': 0.9851459267858992, 'num_leaves': 129}. Best is trial 1 with value: 0.7345446381320461.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 11\u001b[0m with hyperparameters {'feature_fraction': 0.9851459267858992, 'num_leaves': 129} scored 0.7345446381320461 in 0:00:02.194373\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.692558\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[14]\tvalid's auc: 0.734545\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 11 finished with value: 0.7345446381320461 and parameters: {'feature_fraction': 0.9847685553939328, 'num_leaves': 138}. Best is trial 1 with value: 0.7345446381320461.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 12\u001b[0m with hyperparameters {'feature_fraction': 0.9847685553939328, 'num_leaves': 138} scored 0.7345446381320461 in 0:00:02.734485\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.692558\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[14]\tvalid's auc: 0.734545\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 12 finished with value: 0.7345446381320461 and parameters: {'feature_fraction': 0.8418841099372947, 'num_leaves': 177}. Best is trial 1 with value: 0.7345446381320461.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 13\u001b[0m with hyperparameters {'feature_fraction': 0.8418841099372947, 'num_leaves': 177} scored 0.7345446381320461 in 0:00:02.591193\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.691393\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[14]\tvalid's auc: 0.734545\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:optuna.study.study:Trial 13 finished with value: 0.7345446381320461 and parameters: {'feature_fraction': 0.9935669091714021, 'num_leaves': 110}. Best is trial 1 with value: 0.7345446381320461.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 14\u001b[0m with hyperparameters {'feature_fraction': 0.9935669091714021, 'num_leaves': 110} scored 0.7345446381320461 in 0:00:02.161401\n",
            "INFO:lightautoml.ml_algo.tuning.optuna:Hyperparameters optimization for \u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m completed\n",
            "INFO2:lightautoml.ml_algo.tuning.optuna:The set of hyperparameters \u001b[1m{'feature_fraction': 0.8659969709057025, 'num_leaves': 159}\u001b[0m\n",
            " achieve 0.7345 auc\n",
            "INFO:lightautoml.ml_algo.base:Start fitting \u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m ...\n",
            "DEBUG:lightautoml.ml_algo.base:Training params: {'task': 'train', 'learning_rate': 0.05, 'num_leaves': 159, 'feature_fraction': 0.8659969709057025, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'max_depth': -1, 'verbosity': -1, 'reg_alpha': 1, 'reg_lambda': 0.0, 'min_split_gain': 0.0, 'zero_as_missing': False, 'num_threads': 8, 'max_bin': 255, 'min_data_in_bin': 3, 'num_trees': 3000, 'early_stopping_rounds': 100, 'random_state': 42, 'seed': 42}\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.692558\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[14]\tvalid's auc: 0.734545\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.683193\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[13]\tvalid's auc: 0.723882\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.616553\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[30]\tvalid's auc: 0.650807\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.699757\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[12]\tvalid's auc: 0.721847\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:[LightGBM] [Warning] seed is set=42, random_state=42 will be ignored. Current value: seed=42\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 100 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.702363\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[10]\tvalid's auc: 0.742877\n",
            "INFO:lightautoml.ml_algo.base:Fitting \u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.7054033661735845\u001b[0m\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "INFO:lightautoml.ml_algo.base:Start fitting \u001b[1mLvl_1_Pipe_0_Mod_1_CatBoost\u001b[0m ...\n",
            "DEBUG:lightautoml.ml_algo.base:Training params: {'task_type': 'CPU', 'thread_count': 4, 'random_seed': 42, 'num_trees': 3000, 'learning_rate': 0.03, 'l2_leaf_reg': 0.01, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'SymmetricTree', 'max_depth': 5, 'min_data_in_leaf': 1, 'one_hot_max_size': 10, 'fold_permutation_block': 1, 'boosting_type': 'Plain', 'boost_from_average': True, 'od_type': 'Iter', 'od_wait': 100, 'max_bin': 32, 'feature_border_type': 'GreedyLogSum', 'nan_mode': 'Min', 'verbose': 100, 'allow_writing_files': False}\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_1_Pipe_0_Mod_1_CatBoost\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_cb:0:\ttest: 0.7320750\tbest: 0.7320750 (0)\ttotal: 4.46ms\tremaining: 13.4s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:100:\ttest: 0.7442816\tbest: 0.7498383 (19)\ttotal: 193ms\tremaining: 5.53s\n",
            "INFO3:lightautoml.ml_algo.boost_cb:Stopped by overfitting detector  (100 iterations wait)\n",
            "INFO3:lightautoml.ml_algo.boost_cb:bestTest = 0.7498382967\n",
            "INFO3:lightautoml.ml_algo.boost_cb:bestIteration = 19\n",
            "INFO3:lightautoml.ml_algo.boost_cb:Shrink model to first 20 iterations.\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_1_Pipe_0_Mod_1_CatBoost\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_cb:0:\ttest: 0.7144165\tbest: 0.7144165 (0)\ttotal: 2.57ms\tremaining: 7.72s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:100:\ttest: 0.7165474\tbest: 0.7387111 (12)\ttotal: 181ms\tremaining: 5.19s\n",
            "INFO3:lightautoml.ml_algo.boost_cb:Stopped by overfitting detector  (100 iterations wait)\n",
            "INFO3:lightautoml.ml_algo.boost_cb:bestTest = 0.7387111498\n",
            "INFO3:lightautoml.ml_algo.boost_cb:bestIteration = 12\n",
            "INFO3:lightautoml.ml_algo.boost_cb:Shrink model to first 13 iterations.\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_1_Pipe_0_Mod_1_CatBoost\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_cb:0:\ttest: 0.6868870\tbest: 0.6868870 (0)\ttotal: 2.19ms\tremaining: 6.58s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:100:\ttest: 0.7301609\tbest: 0.7321194 (82)\ttotal: 176ms\tremaining: 5.05s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:200:\ttest: 0.7244555\tbest: 0.7332923 (154)\ttotal: 350ms\tremaining: 4.88s\n",
            "INFO3:lightautoml.ml_algo.boost_cb:Stopped by overfitting detector  (100 iterations wait)\n",
            "INFO3:lightautoml.ml_algo.boost_cb:bestTest = 0.7332922894\n",
            "INFO3:lightautoml.ml_algo.boost_cb:bestIteration = 154\n",
            "INFO3:lightautoml.ml_algo.boost_cb:Shrink model to first 155 iterations.\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_1_Pipe_0_Mod_1_CatBoost\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_cb:0:\ttest: 0.7224811\tbest: 0.7224811 (0)\ttotal: 2.26ms\tremaining: 6.77s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:100:\ttest: 0.7345236\tbest: 0.7350968 (69)\ttotal: 193ms\tremaining: 5.55s\n",
            "INFO3:lightautoml.ml_algo.boost_cb:Stopped by overfitting detector  (100 iterations wait)\n",
            "INFO3:lightautoml.ml_algo.boost_cb:bestTest = 0.7350968071\n",
            "INFO3:lightautoml.ml_algo.boost_cb:bestIteration = 69\n",
            "INFO3:lightautoml.ml_algo.boost_cb:Shrink model to first 70 iterations.\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_1_Pipe_0_Mod_1_CatBoost\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_cb:0:\ttest: 0.7246386\tbest: 0.7246386 (0)\ttotal: 2.08ms\tremaining: 6.25s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:100:\ttest: 0.7462105\tbest: 0.7479779 (89)\ttotal: 179ms\tremaining: 5.14s\n",
            "INFO3:lightautoml.ml_algo.boost_cb:Stopped by overfitting detector  (100 iterations wait)\n",
            "INFO3:lightautoml.ml_algo.boost_cb:bestTest = 0.7479778787\n",
            "INFO3:lightautoml.ml_algo.boost_cb:bestIteration = 89\n",
            "INFO3:lightautoml.ml_algo.boost_cb:Shrink model to first 90 iterations.\n",
            "INFO:lightautoml.ml_algo.base:Fitting \u001b[1mLvl_1_Pipe_0_Mod_1_CatBoost\u001b[0m finished. score = \u001b[1m0.658118464291462\u001b[0m\n",
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_1_Pipe_0_Mod_1_CatBoost\u001b[0m fitting and predicting completed\n",
            "INFO:lightautoml.automl.base:Time left 9999999823.68 secs\n",
            "\n",
            "INFO:lightautoml.automl.base:\u001b[1mLayer 2 training completed.\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for test data:\n",
            "array([[0.06539972],\n",
            "       [0.07553179],\n",
            "       [0.05480719],\n",
            "       ...,\n",
            "       [0.06585301],\n",
            "       [0.06425362],\n",
            "       [0.18560858]], dtype=float32)\n",
            "Shape = (2000, 1)\n",
            "Check scores...\n",
            "OOF score: 0.7054033661735845\n",
            "TEST score: 0.7167340353260869\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n09JZdEHvjT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "37YTO3L6xKIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i_iYHBj8xKQB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}